{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6313857f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44c4f53d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>i just feel really helpless and heavy hearted</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ive enjoyed being able to slouch about relax a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>i gave up my internship with the dmrg and am f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i dont know i feel so lost</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i am a kindergarten teacher and i am thoroughl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  label\n",
       "0           0      i just feel really helpless and heavy hearted      4\n",
       "1           1  ive enjoyed being able to slouch about relax a...      0\n",
       "2           2  i gave up my internship with the dmrg and am f...      4\n",
       "3           3                         i dont know i feel so lost      0\n",
       "4           4  i am a kindergarten teacher and i am thoroughl...      4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "\n",
    "df = pd.read_csv('text.csv')[0:20000]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c52144b",
   "metadata": {},
   "source": [
    "#### Text Preprocessing\n",
    "1) Lowercasing\n",
    "2) Removing Punctuation and Special Characters\n",
    "3) Tokenization - Splitting the individual text sentences into a string of tokens, where each token represents a particular subset of the sentence.\n",
    "4) Removing Stop Words - Removing common words in the English language that won't provide much value.\n",
    "5) Stemming - Reducing words to their base form.\n",
    "6) Encoding - Converting categorical labels into a numerical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b49c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/esvaranarun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/esvaranarun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      processed_text  label\n",
      "0                 feel really helpless heavy hearted      4\n",
      "1  ive enjoyed able slouch relax unwind frankly n...      0\n",
      "2            gave internship dmrg feeling distraught      4\n",
      "3                                dont know feel lost      0\n",
      "4  kindergarten teacher thoroughly weary job take...      4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "# Download NLTK data (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialize stop words - Removing commonly used words in English sentences\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# print(stop_words)\n",
    "\n",
    "# Function to preprocess text - Takes in each individual sentence\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation and special characters\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Removing stop words\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Apply preprocessing to the text column - Column-wise operation across the entire dataset\n",
    "data['processed_text'] = df['text'].apply(preprocess_text)\n",
    "data['label'] = df['label']\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d73f20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a vocabulary and encode text\n",
    "vocab = set(token for text in data['processed_text'] for token in text)\n",
    "word2idx = {word: i+1 for i, word in enumerate(vocab)}  # Start indexing from 1\n",
    "\n",
    "def encode_text(text):\n",
    "    return [word2idx[word] for word in text]\n",
    "\n",
    "data['encoded_text'] = data['processed_text'].apply(encode_text)\n",
    "\n",
    "# Pad sequences to ensure uniform length\n",
    "MAX_LEN = max(len(text) for text in data['encoded_text'])\n",
    "data['padded_text'] = data['encoded_text'].apply(lambda x: x + [0]*(MAX_LEN - len(x)))\n",
    "\n",
    "# Split into training and testing datasets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = torch.tensor(texts, dtype=torch.long)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = TextDataset(train_data['padded_text'].tolist(), train_data['label'].tolist())\n",
    "test_dataset = TextDataset(test_data['padded_text'].tolist(), test_data['label'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a83c06b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227620e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 129206\n"
     ]
    }
   ],
   "source": [
    "# Define a more complex model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "class ImprovedTextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_classes):\n",
    "        super(ImprovedTextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embed_size]\n",
    "        _, (hn, _) = self.lstm(x)  # [1, batch_size, hidden_size]\n",
    "        x = hn.squeeze(0)  # [batch_size, hidden_size]\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        out = self.fc2(x)\n",
    "        return out\n",
    "\n",
    "    \n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab) + 1  # +1 for padding index\n",
    "EMBED_SIZE = 100\n",
    "HIDDEN_SIZE = 128\n",
    "NUM_CLASSES = 6  # Example number of classes\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = ImprovedTextClassificationModel(VOCAB_SIZE, EMBED_SIZE, HIDDEN_SIZE, NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Total number of parameters: {count_parameters(model)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c8928e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 1.5695\n",
      "Epoch [2/20], Loss: 1.5668\n",
      "Epoch [3/20], Loss: 1.5670\n",
      "Epoch [4/20], Loss: 1.5660\n",
      "Epoch [5/20], Loss: 1.5664\n",
      "Epoch [6/20], Loss: 1.5669\n",
      "Epoch [7/20], Loss: 1.5664\n",
      "Epoch [8/20], Loss: 1.5664\n",
      "Epoch [9/20], Loss: 1.5666\n",
      "Epoch [10/20], Loss: 1.5662\n",
      "Epoch [11/20], Loss: 1.5660\n",
      "Epoch [12/20], Loss: 1.5653\n",
      "Epoch [13/20], Loss: 1.5655\n",
      "Epoch [14/20], Loss: 1.5655\n",
      "Epoch [15/20], Loss: 1.5657\n",
      "Epoch [16/20], Loss: 1.5657\n",
      "Epoch [17/20], Loss: 1.5654\n",
      "Epoch [18/20], Loss: 1.5657\n",
      "Epoch [19/20], Loss: 1.5654\n",
      "Epoch [20/20], Loss: 1.5653\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "615cb34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a267b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
